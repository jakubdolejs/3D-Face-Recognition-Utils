# 3D face recognition based on FR3DNet

## Installation

We recommend setting up the project in a virtual environment.

```
python3 -m venv .venv
source .venv/bin/activate
```
The above command creates and activates a virtual environment in a hidden folder called **.venv**. From now on, dependencies won't be installed globally but will be scoped to the virtual environment.

### Dependencies

The project depends on the [`image3d_utils`](https://github.com/AppliedRecognition/3D-Image-Utils-Python) library. Follow the instructions in the above link to build and install the library. After building the library, install it using:

```
pip install /path/to/3D-Image-Utils-Python
```

## API

To detect faces in an image/face package use the [`FaceRecognition`](./face_recognition_fr3dnet/face_recognition.py) class. The class expects a path to the [fr3dnet.pt model file](./models/fr3dnet.pt) as its constructor parameter.

```python
from face_recognition_fr3dnet import FaceRecognition

# Path to the model file (change to real path)
model_file_path = "/path/to/fr3dnet.pt"

# Path to image/face package
image_package_path = "/path/to/image-face.bin"

# Create instance of the face recognition class
recognition = FaceRecognition(model_file_path)

# Create face template
template = recognition.create_face_template(image_package_path)

# Get templates you want to compare
templates_to_compare = [template1, template2] # Created the same way as template (above)

# Run face template comparison
scores = recognition.compare_face_templates(template, templates_to_compare)

# The scores range from 0.0 to 1.0
# The scores list has the same order as templates_to_compare:
# score[0] is the score between template and template1
# score[2] is the score between template and template2
```

## Tests

To run unit tests execute:

```
python3 -m pytest
```

## Face template creation description

The 3D pointcloud of each scan in the training data is used to generate a three channel image. 

- The first channel is the depth image which is generated by fitting a surface of the form z(x,y) to the 3D pointcloud using the gridfit algorithm. 
- The surface normals of the original pointcloud are calculated in spherical coordinates (θ, φ) where θ, φ are the azimuth and elevation angles of the normal vector. 
- Using a similar x, y grid to the depth image, surfaces of the form θ(x, y) and φ(x, y) are fitted to the azimuth and elevation angles to make the second and third channels of the 3D image representation we used to train our network. 
- The three channels are normalized on the 0-255 range and can be rendered as an RGB image. 
- This image is passed through a landmark identification network to detect the nosetip. With the face centered at the nosetip, we crop a square of 224 × 224 pixels. 
- These images are down-sampled to 160 × 160 for use in our network.